Mini-Batch Gradient Descent
. split your training examples into smaller batches and perform gradient descent after forward prop on each batch

Notation Review
. x^(i) -- i'th training example
. z^[l] -- l'th layer of neural network
. X^{t} -- t'th mini-batch
. "epoch" -- one pass through entire training set; so, with mini-batch gd you're running gradient descent t times for a single epoch, where t is number of mini-batches
