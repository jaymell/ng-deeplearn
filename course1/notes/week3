Neural Networks

Notation
. a^[0] = x -- [0] corresponds to layers -- x is effectively layer 0
  -- in 2-layer network, x = 0, hidden layer = 1 (a^[1]), output layer = a^[2] == yhat
  -- if hidden layer had 4 nodes, these would be a^[1]_1, a^[1]_2 ... a^[1]_4
  -- w^[1] would be w matrix for hidden layer -- 4x3 matrix -> 4 nodes in layer, 3 input units
  -- b^[1] would be 4x1 matrix -> corresponding to 4 nodes of layer
. g^[l] = particular activation function used at layer l -- e.g., sigmoid, tanh

. continuing the above example:
 -- z^[1]_1 = w^[1]_1'x + b^[1]_1 -> a^[1]_1 = sigmoid(z^[1]_1)
   -- w in this case is row vector 1x3 after being transposed
   -- x would be row vector 1x3 of weights from each x of previous layer
   -- b a single real number
    ... for each of 4 nodes of 1st layer
 -- this can be condensed into single matrix operation:
   Z^[1] = W^[1]'x + b^[1]
   -- x is still just a column vector b/c its values don't change, only their weights
      as applied to each node of next layer
-- a^[1] = sigmoid(z^[1]), a^[1] = 4x1 matrix, z^[1] = 4x1
-- output layer:
  z^[2] = W^[2]a^[1] + b^[2],
  -- z^[2] is 1x1, W^[2] is 1x4, a^[1] is 4x1, b^[2] is 1x1
  -- a^[2] = simoid(z^[2]), 1x1

Vectorizing Across Multiple Training Examples
. Z = WX + b, A = sigmoid(Z)
 -- Z becomes [# of input units]Xm matrix with individual vectors stacked in columns
 -- X becomes nXm matrix of training examples stacked in columns
 -- A becomes [# of hidden units]Xm matrix with individual values stacked in columns

Activation Functions
. ie, sigmoid function -- a = 1/(1+e^-z)
. tanh -- (e^z - e^-z) / (e^z + e^-z)
 -- centers data around 0
 -- a and z range between -1 and 1
 -- not appropriate for binary classification, though b/c of -1 to 1
. activations can be different at different layers, so sigmoid for output layer, others for other layers
. downside to both tanh and sigmoid:
  -- if z is very large or small, slope is very small, so training will take a long time
. relu -- rectified linear unit:
 -- a = max(0, z)
 -- increasingly becoming the default
 --  leaky relu is an alternative, derivative won't be 0 if z is negative
   max(0.01, z)
. linear activation functions generally never used, except in very specific circumstances,
  like output layer of example where output is a real number

Gradient Descent
. for 2-layer NN
. J(W^[1], b^[1], W^[2], b^[2]) = 1/m Sigma L(a^[2], y)
1) Random initialization of parameters
2) Compute a^[2]_i for (i=1, ... m)
   Computer dJ/dW^[1] aka dW^[1], db^[1], dW^[2], db^[2]
   Update parameters:
     W^[1] = W^[1] - alpha * dW^[1]
     ... etc.

. Forward propagation
  Z^[1] = W^[1]X + b^[1]
  A^[1] = g^[1](Z^[1])
  ... etc

. Vectorized Backward Propagation
  dZ^[2] = A^[2] - Y
  dW^[2] = 1/m (dZ^[2]A^[1]') -- A^[1] is transposed
  db^[2] = 1/m np.sum(dZ^[2], axis=1, keepdims=True)  -> n^[2] X 1 matrix
  dZ^[1] = W^[2]' * dZ^[2] * g^[1]'(Z^[1])
                  |              |
           n^[1] X m matrix      |
                               n^[1] X m matrix
  dW^[1] = 1/m(dZ^[1]X') -- X is transposed
  db^[1] = 1/m (np.sum(dZ^[1], axis=1, keepdims=True) -> n^[1] X 1 matrix

. Parameter initialization
 -- for NNs, must be randomly initialized
 -- actually, initializing biases to 0 is ok, but not W
 -- if initialized to 0, all units in layers will be identical forever
 -- W^[1] = np.random.rand((dimension)) * 0.01
   -- small value (0.01) is chosen to avoid large z values, and thus, slow learning
 -- b^[1] = np.zero((dimension))


# matrix multiplication in numpy:
a = np.array([[1,2],[1,2],[1,2],[1,2]])
b = np.array([1,2])
c = np.dot(a,b)
