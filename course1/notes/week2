Notation
. n_x -- input size of x
. X matrix -- each individual test case is a column
 -- shape (n_x, m)
. Y matrix -- also columnar
 -- shape (1, m)

Logistic Regression
. y_hat = sigmoid( W*x + b )
  -- W is n_x dimensional vector
  -- b is real number
. sigmoid = 1 / (1 + e^-z)
. think of z as the x axis on a sigmoid graph -- large z makes denom small -> close to 1
  -- small z make denom large -- close to 0
  -- goal is for params W and b to lead to accurate values of sigmoid function given inputs

. Loss function
 -- squared error not sufficient with logistic regression (e.g., 1/2(y_hat - y)^2)
 -- instead:
  L(y_hat, y) = -(ylog y_hat + (1 -y) log(1-y_hat))
 -- if y_hat is 1, y will ideally be large as well ( - log y_hat )
 -- if y_hat is 0, y will ideally be small ( - log (1 - y_hat ))

. Cost function
 -- summation of loss function across entire data set
 -- J (w,b) = -1/m Sigma[ y^i log y_hat^i + (1 -y^i) log (1 - y_hat^i ) ]

. Gradient Descent
 -- updates values of w and b based on derivative
 -- w := w - alpha * d J(w, b) / dw
  -- aka w - alpha * "dw"
 -- b := b - alpha * d J(w, b) / db

Derivatives 
 -- effectively, the slope of a function / rate of change of a function at a particular point
 -- f(x) = x^2 ... d/dx f(x) = 2x
 -- f(x) = x^3 ... d/dx f(x) = 3x^2
 -- f(x) = log x ... d/dx f(x) = 1/x

Chain Rule of Calculus

 u=bc -> 
  v = a + u ->
    J = 3v

dJ/dv = 3 -- J increase by 3x whatever v increases by
dV/da = 1

. chain rule says that impact of a on J will be product of 
  a's impact on v and v's impact on J

  == 3 * 1
  == 3

Notation: 
  a (activation) == y_hat
  z == result of your function prior to sigmoid 
   -- e.g., with 2 variables x1 and x2, z = w1x1 + wx2x + b
  a = sigma(z)
  L = loss(a, y)
.... *** shortcut: dz (derivative of L with respect to z) = a - y ***
  for single training example:
    dw1 = w1*dz
    dw2 = w2*dz
    db = dz

Vectorizing LR
. Think of vectorized x as row vector of all individual values of z per training set
. [ z1, z2, ... zm ] == w' X + [b, b ... b ] -> b's are row vector of individual biases
 X = n * m vector -- n is number of x variables in a single example, m is number of training examples
 w = row vector of weights -- [w1, w2 ... wn ]
 A = row vector of individual a values
 Z = row vector of individual z values
 Y = row vector of individual y values
. in python:
  Z = np.dot(W.T, x) + b
  -- python will automatically turn b into a row vector -- 'broadcasting'

. gradient descent:
 dz1 = a1-y1 , dz2 = a2-y2, etc...
 dZ = [ dz1, dz2 ... dzm ]
 dZ = A - Y
 db = 1/m * Sigma(dZ) -- in python, np.sum(dZ) * 1/m
 dw = 1/m X*dZ'

Broadcasting -- python will auto-copy real numbers and small matrices to appropriate dimensions for doing
  matrix arithmetic

