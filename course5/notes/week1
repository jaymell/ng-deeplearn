RNNs

example: Named Entity Recognition
given a sentence: "Harry Potter and Hermione Granger invented a new spell"
give a y indicating whether each word is part of a name: 1 1 0 1 1 0 0 0 0

notation:
  "Harry Potter and Hermione Granger invented a new spell"
  x^<1>  x^<2>  x^<3>   ...          x^<t>   ...    x^<9>
  Tx = 9

  1   1   0   1   1   0   0   0   0
  y^<1>    ....       y^<t>  ...  y^<9>
  Ty = 9

NLP
. come up with a 'vocabulary' or 'dictionary'
  -- list of all possible words -- number each word in dictionary
  -- modern dictionaries often >1 million words
. one-hot representation --- ie vector of 9999 zeroes and one 1 for a particular
  word in a 10,000 word dictionary

Why not use a Standard NN?
. inputs, outputs can be different lengths, ie from sentence to sentence
  -- you could potentially zero-pad shorter sentences
. But, still wouldn't learn from commonalities appearing in different parts of sentence
  -- ie, 'Harry' appearing as first word wouldn't generalize to Harry appearing later


Basic structure
. with each loop or time-step of RNN, your input is both x and a
 -- x is whatever value of x is for particular iteration
 -- a is an activation based on previous iterations; a_0, along with x^<1> is first
    input to RNN, a_0 usually just a zero-vector
 -- W_ax are weights applied at each time-step of the network
  -- shape: (n_a, n_x)
 -- W_aa are weights applied to the a matrices at each time-step of network
   -- shape: (n_a, n_a)
 -- W_ya are weights applied to y at each time-step
   -- shape: (n_y, n_a)
 -- b_a
   -- shape: (n_a, 1)
 -- b_y
   -- shape: (n_y, 1)
 -- the double sub-scripts indicate that, for example for W_ax, it is being used
    to compute an a-like and x-like quantity ...expl?
. RNNs can be either unidirectional or bidirectional
 -- with unidirectional, only weights or prior values contribute to subsequent input values
 -- with bidirectional, later values of x can also contribute at a given time-step
   -- e.g., imagine using later words of a sentence as weighted input for an earlier word

. calculations:
 -- at time-step 0:
   a^<0> = 0
 -- at time-step 1:
   a^<1> = g(W_aa*a^<0> + W_ax*X^<1> + b_a) -- g is often either tanh or relu
   yhat^<1> = g(W_ya*a^<1> + b_y) -- y is generally calculated using sigmoid or softmax for multi-class
 -- at time-step t:
   a^<t> = g(w_aa * a^<t-1> + W_ax*X^<t> + b_a)
   yhat^<t> = g(W_ya*a^<t> + b_y)

. forward prop:
  -- basic idea is that a^<0> and x^<1> are used to calculate y^<1> and a^<1>
  -- a^<1> and x^<2> are used to calculate y^<2> and a^<2>
  -- a^<t-1> and x<t> calculate y^<t> and a<t>

simplifying the parameters a bit to make dealing with deep networks easier:
. from this:
   a^<t> = g(w_aa * a^<t-1> + W_ax*x^<t> + b_a)

  to this:

  a^<t> = g(w_a[ a^<t-1>,x^<t> ] + b_a )

  collapse w_aa and w_ax into a single matrix:

  [ w_aa ; w_ax ] = W_a

  ie, if w_aa is 100x100 matrix and w_ax is 100x10000, Wa is 100x10100 dimensional


  colapse a^<t-1> and x^<t> into a single vector;, so if a^<t-1> is 100x1 and x^<t> is 10000x1:

  [ a^<t-1>,x^<t> ] is 10100x1 vector


  if you multiply these collapsed matrices, you will get back the original values:

  [ w_aa ; w_ax ] x [ a^<t-1>,x^<t> ]  = w_aa * a^<t-1> + W_ax*x^<t>


  this doesn't change much:

  yhat^<t> = g(W_ya*a^<t> + b_y)

  just simplify the subscript:

  yhat^<t> = g(W_y*a^<t> + b_y)


RNN Backpropagation
. L(yhat, y) = Sigma_1..Tx( L^<t>(yhat^<t>, y^<t> ))
. calculations of individual losses look like logistic regression:
  L^<t>(yhat^<t>, y^<t> ) = -y^<t> * log(yhat^<t>) - (1 - y^<t>) * log(1-yhat^<t>)


Various RNN architectures
. many-to-many
 -- ie input is a sequence of words and output is a sequence of words
 -- output dimensions don't have to much, though; imagine translating between 2 languages
   -- so multiple recursions of input sequence ("encoder") followed by multiple recursions
      of output feeding more output ("decoder")

. many-to-one
 -- ie input is a series of words
 -- output can be a single number in 0-5 scale, ie for sentiment classification
 -- in this case, the network itself recurses many times before outputting a single value

. one-to-one
 -- more like a standard NN

. one-to-many
 -- music generation
 -- single input corresponding to output of multiple notes
 -- in reality, input would just be output of previous layer, without separate x inputs *** (a?)


Language / Sequence Modeling
. first tokenize sentences, ie one-hot encode words based on vocabulary; possible
  tokens for EOS (end of sentence) and unknown words

. structure of RNN, given a sentence input:
 -- goal is basically to predict next words, given initial 0 input followed by actual
    words of sentence

"This random sentence"

t1:
a^<0> = 0
x^<1> = 0
... y^<1> = P(any word in dictionary being first word) -- softmax

t2:
a^<1> = ...
x^<2> = y^<1> (whatever the first word _actually_ is), in this case, "This"
y^<2> = P(2nd word | "This")

and so on....

key point is that x input is always, with exception of first iteration, the _actual_ values
of y

. Loss
 L(yhat^<t>, y^<t>) = -Sigma(y_i^<t> * log(yhat_i^<t>) )

. Overall Los
 Sigma( L^<t>(yhat^<t>, y^<t>) )


Generating Novel Sequences
. Given a vocabulary, provide t1 input as above (0's initially); yhat^<1> output a vector, make
a random choice from that (e.g., np.random.choice), use that as input for x^<2>, and so on
. You can also do this with character level sequence instead of words
 -- e.g., vocabulary of alphabet, special characters, digits, etc.
 -- pro: won't have to worry about unknown words, since can assign probability to any sequence of characters
 -- models more computationally expensive and not as good at capturing long-range dependencies


Vanishing Gradients
. Imagine a sentence with long-range dependencies.... e.g.,

The _dogs_, which were ...., _were_
The _dog_, which was ...., _was_

Problem is similar to deep neural networks, which is that it's difficult for backpropagation beginning at later t-values (layers) to affect earlier t-values (layers)

. exploding gradients is also possible, and catastrophic for model, but can be fixed via 'gradient clipping'


Gated Recurrent Unit
. is actually more recent (and less complicated) than LSTM
. memory cell
. stores memory about particular t's of network
. potentially updated at each t

c^<t> = a^<t> -- think of c as 'candidate' -- ie, it _may_ be used to update
each iteration:
ctilde^<t> = tanh(W_c[c^<t-1>, x^<t>] + b_c)

Update Gate -- between 0 and 1 -- think of it as being 0 or 1, though can be between:
  Gamma_u = sigmoid(W_u[c^<t-1>, x^<t>] + b_u) -- u means 'update'

Value of gate determines whether we update c with ctilde:

c^<t> = Gamma_u * ctilde^<t> + (1-Gamma_u) * c^<t-1>

if Gamma_u is 0, c^<t> = c^<t-1>
if Gamma_u is 1, c^<t> = ctilde^<t>


c^<t> can be a vector, in which case Gamma and ctilde are same dimension
if vector, multiplication in updating c<t< is _element-wise_, so some 'bits' in the
vector change, others don't


Actually, above is oversimplifcation. There is actually an additional Gamma_r involved
 -- think of the r as being 'relevance'

 ctilde^<t> = tanh(W_c[Gamma_r, c^<t-1>, x^<t>] + b_c)
 Gamma_u = sigmoid(W_u[c^<t-1>, x^<t>] + b_u)
 Gamma_r = sigmoid(W_r[c^<t-1>, x^<t>] + b_r)
 c^<t> = Gamma_u * ctilde^<t> + (1-Gamma_u) * c^<t-1>


 Alternate notation:
 htilde for ctilde^<t>
 u for Gamma_u
 r for Gamma_r
 h for c^<t>


LSTM -- Long Short-Term Memory Unit

ctilde^<t> = tanh(W_c[a^<t-1>, x^<t>] + b_c) -- a^<t-1> instead of c^<t-1>
Gamma_u = sigmoid(W_u[a^<t-1>, x^<t>] + b_u)
Gamma_f = sigmoid(W_f[a^<t-1>, x^<t>] + b_f) -- 'forget' gate
Gamma_o = sigmoid(W_o[a^<t-1>, x^<t>] + b_o) -- 'output' gate
c^<t> = Gamma_u * ctilde^<t> + Gamma_f*c^<t-1> -- element-wise multiplication
a^<t> = Gamma_o*c^<t>

